# -*- coding: utf-8 -*-
"""D3_ReadData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y1bNXB-hUt2dZ5TNRNReYgBESBBcm_6q
"""

from google.colab import drive
drive.mount("/content/gdrive")

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/My\ Drive/Data
!ls

import pandas as pd
import numpy as np
import nltk 
from glob import glob
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

nltk.download('stopwords')

archive_dir = np.array(glob('archive/*'))
print(archive_dir)

data = pd.read_csv('archive/covid_19_data.csv')
print(data.columns)
data.head()

open_line_list = pd.read_csv('archive/COVID19_open_line_list.csv')
print(open_line_list.columns)
open_line_list.head()

open_line_list.isnull().sum()

# open_line_list.dropna(subset=['province'],inplace=True)

open_line_list[['travel_history_location','province']] = open_line_list[['travel_history_location','province']].fillna(value = 'None')

open_line_list.isnull().sum()

print(open_line_list.shape)

nodes = Counter(open_line_list['province'].values)

nodes_index = {}
index_nodes = {}

for i, (node,_) in enumerate(nodes.items(),start = 1):
  nodes_index[node.lower()] = i
  index_nodes[i] = i

print(len(nodes))

lst = []
for node in nodes_index.keys():

  lst.append([nodes_index[node],node.capitalize()])

nodesdf = pd.DataFrame(lst,columns = ['id','label'])
# nodesdf.to_csv('open_line_nodes.csv')

travel_history = Counter(open_line_list['travel_history_location'].values)
print(travel_history)

stop_words = set(stopwords.words('english')) 
tokenizer = RegexpTokenizer(r'\w+')

print(len(open_line_list))

def get_edge(source,destination):
  try:
    return[nodes_index[source.lower()],nodes_index[destination.lower()]]
  except:
    return None

print(open_line_list.shape)

edges = []
error_count = 0
for i, row in open_line_list.iterrows():
  # try:
    print(i)  
    province, history = row['province'], row['travel_history_location']
    
    tokenized = tokenizer.tokenize(history)
    tokenized = [x for x in tokenized]

    if 'via' in tokenized:
      v = tokenized.index('via')

      e1 = get_edge(tokenized[0],tokenized[v+1])

      if e1 != None:
        edges.append(e1)

      e2 = get_edge(tokenized[v+1],province)

      if e2 != None:
        edges.append(e2)

    else:
      e = get_edge(tokenized[0],province)

      if e != None:
        edges.append(e)

  # except:
  #   error_count += 1
  #   print("{:} edges of {:} edges failed".format(error_count,i+1))

print(len(edges))

def unique(lst):
  l = []
  count =0
  for x in lst:
    if x in l:
      continue
    else:
      l.append(x)
      count += 1
  
  return count

xx = Counter(open_line_list['travel_history_location'].values)
print(xx)

print(unique(edges))

print(edges)

edgesdf = pd.DataFrame(edges,columns = ['source','target'])
edgesdf.to_csv('open_line_edges.csv',index = False)

lst = []
for node in nodes_index.keys():

  lst.append([nodes_index[node],node.capitalize()])

nodesdf = pd.DataFrame(lst,columns = ['id','label'])
nodesdf.to_csv('open_line_nodes.csv',index = False)

nodesdf.head()

print(data.head())

nodesdf['confirmed'] = nodesdf.apply(lambda row : data[data['Province/State'] == row['label']].Confirmed.sum(), axis = 1)
nodesdf['deaths'] = nodesdf.apply(lambda row : data[data['Province/State'] == row['label']].Deaths.sum(), axis = 1)
nodesdf['recovered'] = nodesdf.apply(lambda row : data[data['Province/State'] == row['label']].Recovered.sum(), axis = 1)
nodesdf.head()

nodesdf.to_csv('nodes.csv',index = False)